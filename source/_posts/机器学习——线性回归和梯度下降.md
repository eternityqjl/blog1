---
title: 机器学习——线性回归和梯度下降
date: 2021-04-23 18:10:29
tags: [机器学习,线性回归,梯度下降]
categories: 机器学习
mathjax: true
---

## 线性回归Linear Regression

### 简介

线性回归属于监督学习，先给定一个训练集根据训练集学习出一个线性函数，然后测试这个函数训练的好不好，挑选出最好的函数（cost function最小）即可。

注意：

* 因为是线性回归，所以学习到的函数为线性函数，即一次直线函数；
* 因为是单变量，所以只有一个x；

所以能够给出**单变量线性回归**的模型：
$$
h(x)=b+mx
$$
我们称$x$为feature，$h(x)$为hypothesis。

<!-- more -->

### 代价函数Cost Function

我们需要根据代价函数来确定线性回归拟合的好不好。Cost Function越小，说明线性回归地越好，最小为0，完全拟合。
$$
J(b,m)=\frac{1}{2n}(h(x^{(i)})-y^{(i)})^2
$$
如上所示为代价函数的构造，其中，$x^{(i)}$表示向量x的第i个元素，$y^{(i)}$表示向量y的第i个元素，即表示所有输入的训练集的点。$h(x)$表示已知的假设函数，n为训练集的个数。

## 梯度下降

梯度下降能够找出代价函数Cost Function的最小值，梯度下降的方法步骤如下所示：

1. 先确定向下一步的步伐大小，我们称为Learning Rate；
2. 任意给定初始值b, m；
3. 确定一个向下的方向，按预定步骤向下走，并且更新b, m；
4. 当下降高度(循环次数)小于某个定义的值时，停止下降。

循环下面的式子直到满足终止条件：
$$
b = b - \alpha\frac{\partial}{\partial b}J(b,m) \\
m = m - \alpha\frac{\partial}{\partial m}J(b,m)
$$
上式中的$\alpha$为Learning rate，决定了下降的步伐大小；偏导数决定了下降的方向。

### 对Cost Function运用梯度下降

$$
\mathrm{repeat \ until \ convergence}\{ \\
b:=b-\alpha \frac{1}{n}\sum_{i=1}^{n}(h(x^{(i)})-y^{(i)}) \\
m:=m-\alpha \frac{1}{n}\sum_{i=1}^{n}(h(x^{(i)})-y^{(i)})x^{(i)} \\
\\ \}
$$

设定好初始值、循环次数、Learning Rate后即可开始拟合，最终得到结果b,m；得到拟合的线性函数。

## 代码实现

### 使用`sklearn`库中的`make_regression`函数生成训练数据

```python
#generate the train data
def geneData():
    points = []
    xSet, ySet = make_regression(n_samples=100, n_features=1, n_targets=1, noise=20)
    for x,y in zip(xSet,ySet):
        x=x[0]
        point = [x,y]
        points.append(point)
    return points
```

### 定义代价函数

```python
def costFunction(b, m, points):
    for point in points:
        ErrorTotal += ((b + m*point[0]) - point[1]) ** 2
    return ErrorTotal / (2 * float(len(points)))
```

### 定义梯度下降函数

```python
def stepGradient(b_current, m_current, b_gradient, m_gradient, points, learningRate):
    N = float(len(points))
    for point in points:
        x = point[0]
        y = point[1]
        b_gradient += (2/N) * ((b_current + m_current * x) - y)
        m_gradient += (2/N) * x * ((b_current + m_current * x) - y)
    new_b = b_current - (learningRate * b_gradient)
    new_m = m_current - (learningRate * m_gradient)
    return [new_b, new_m, b_gradient, m_gradient]
```

### 设置初始化参数并循环迭代，最后画出结果

```python
if __name__ == '__main__':
    iterations = 100    #迭代次数
    learningRate = 0.0001   #学习率，决定了下降的步伐大小
    points = geneData() #生成训练集
    b = 0   #线性方程参数b,m的初始值
    m = 0   
    b_gradient = 0  #代价函数梯度下降结果的初始值
    m_gradient = 0
    for i in list(range(1, iterations+1)):  #循环进行梯度下降，求得结果
        b,m,b_gradient,m_gradient = stepGradient(b,m,b_gradient,m_gradient,points,learningRate)

    for point in points:    #画出样本点
        plt.scatter(point[0], point[1])

    #画出得到的直线
    t = np.arange(-3,3,0.01)
    s = b + m * t
    plt.scatter(t,s,linewidths=0.5)
    plt.show()

    #输出结果
    print("b=%f"%b)
    #print(b)
    print("m=%f"%m)
```

## 结果展示

![Gradient_figure1](https://raw.githubusercontent.com/eternityqjl/blogGallery/master/blog/Gradient_figure1.png)

```python
b=4.824787
m=46.528725
```

## 哲学思考

梯度下降是一个一阶最优化算法，通常也称为最陡下降法，要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度的反方向规定的步长距离点进行迭代搜索。